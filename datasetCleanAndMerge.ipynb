{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244a3a1b-3dcc-409c-b624-72ab7d656986",
   "metadata": {},
   "source": [
    "# cleanPosts() Function:\n",
    "Args:\n",
    "- Ensure that all input data for the \"posts\" argument as a single column.\n",
    "\n",
    "Functionality:\n",
    "- Disposes of all non-post columns/features of the data (i.e. author, url, score, num_comments, etc.)\n",
    "- Disposed of all empty rows/posts.\n",
    "- Disposed of rows that contained [deleted] and [removed] posts.\n",
    "- All posts are truncated to 512 words (delimited by a space).\n",
    "\n",
    "Returns:\n",
    "- Returns a Pandas DataFrame containing the cleaned dataset\n",
    "- Also returns the number of posts that needed truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad8afc9-1516-404d-9b6b-3943f5294b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanPosts(posts, newColumnLabel:str):\n",
    "    posts = posts[posts != '[deleted]'] #Filter out rows where the value is [deleted]\n",
    "    posts = posts[posts != '[removed]'] #Filter out rows where the value is [removed]\n",
    "    posts = posts.dropna() #Remove rows where the post is empty\n",
    "\n",
    "    originalLengths = posts.apply(lambda x: len(x.split())) #A list of the original length of each post.\n",
    "    truncatedCount = sum(originalLengths > 512) #Calculate the number of posts that will be truncated to 512 words\n",
    "    \n",
    "    posts = posts.apply(lambda x: ' '.join(x.split()[:512])) #Truncate number of words in the post to 512  \n",
    "    posts = posts.reset_index(drop=True) #Reset the indices by dropping all removed rows from the list\n",
    "    \n",
    "    return pd.DataFrame({newColumnLabel: posts}), truncatedCount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e59a39f-5815-4254-ac1e-2df6c50c698d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f739103-1f4a-4ac6-bfd9-2bbab91e14a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18051337-62f4-42f6-9b3e-5f8ac523e636",
   "metadata": {},
   "source": [
    "# Positive Class CSV Creation\n",
    "- Outside of the code, converted the original .xlsx source Excel spreadsheet to a utf-8 encoded CSV file to remove right quotation symbols (â€™) from the text.\n",
    "- 985 of the 13697 PTSD-positive posts were truncated to 512 words (delimited by a space) during this data cleaning. This value is contained in posTruncatedCount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f60d97-97fa-43a6-964d-03507f6c19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the original CSV file\n",
    "df = pd.read_csv('raw_datasets/positive/rptsd.csv', delimiter=',')\n",
    "\n",
    "posts = df.iloc[:, 1] #Only need to work with the posts themselves. Dispose of all other author, url, and other related data.\n",
    "positive_df, posTruncatedCount = cleanPosts(posts, 'posts')\n",
    "\n",
    "#Create a new CSV file with the newly organized data\n",
    "positive_df.to_csv('positive_cleaned.csv', index=False)  #Set index=False to exclude row numbers in the output file\n",
    "\n",
    "numPositive = len(positive_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c16f19-b8be-48c1-a639-5630e416e1aa",
   "metadata": {},
   "source": [
    "# Negative Class CSV Creation\n",
    "- This class was built using a combination of non-PTSD related subreddits (r/fitness, r/teaching, etc.)\n",
    "- Each subreddit has its own CSV data file.\n",
    "- The number of positive instances is calculated above, and this value is used to determine how many negative instances should be taken from each subreddit (so that there is roughly equal representation - both between the positive and negative classes as well as equal representation between the various negative subreddits).\n",
    "- generateFullNegativeClassDatasets should be set to False for the actual research. Setting this to True will simply generate cleaned CSVs for the entire dataset, rather than a random sample.\n",
    "- The same cleaning procedure was performed on each of these subreddits.\n",
    "- 676 of the total 13696 negative posts were truncated to 512 words (delimited by a space) during this data cleaning. This value is contained in negTruncatedCount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cbb9ccb-522c-45d5-9a4e-0111f3fd21aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "generateFullNegativeClassDatasets = False #If you set this to True, it will generate a massive cleaned dataset of ALL negative occurrences across all CSV files. Setting to False will make positive and negative instances even.\n",
    "negTruncatedCount = 0 #Sums the overall number of negative instances that needed to be truncated across all datasets.\n",
    "\n",
    "directory = 'raw_datasets/negative' #Directory containing negative CSV files\n",
    "\n",
    "dfs = [] #List to store DataFrames from each CSV file\n",
    "\n",
    "listOfNegativeCSVFiles = [f for f in os.listdir(directory) if f.endswith('.csv')] #List of all CSV files\n",
    "numNegativeCSVFiles = len(listOfNegativeCSVFiles)\n",
    "numNegPerClass = round(numPositive / numNegativeCSVFiles) #Number of instances to sample from each negative CSV file.\n",
    "\n",
    "#Iterate through each CSV file in the directory\n",
    "for filename in listOfNegativeCSVFiles:\n",
    "    filepath = os.path.join(directory, filename)\n",
    "\n",
    "    df = pd.read_csv(filepath, delimiter=',') #Read from the original CSV file and create a DataFrame\n",
    "    if generateFullNegativeClassDatasets == False:\n",
    "        df = df.sample(n=numNegPerClass, random_state=42) #Sample only the same number of observations from each negative subreddit\n",
    "    \n",
    "    posts = df.iloc[:, 3] #Only need to work with the posts themselves. Dispose of all other data columns.\n",
    "    negative_df, partialNegTruncatedCount = cleanPosts(posts, 'posts')\n",
    "    negTruncatedCount += partialNegTruncatedCount #Sum up the total number of negative posts that needed truncation.\n",
    "    \n",
    "    #Create a new CSV file with the newly organized data\n",
    "    if generateFullNegativeClassDatasets == True:\n",
    "        negative_df.to_csv('full_negative_cleaned/' + filename, index=False)\n",
    "    else:\n",
    "        negative_df.to_csv('partial_negative_cleaned/' + filename, index=False)\n",
    "        \n",
    "    dfs.append(negative_df) #Append the DataFrame to the list for the total dataset concatenation later.\n",
    "\n",
    "#Concatenate the DataFrames into one DataFrame\n",
    "negative_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "negative_df.reset_index(drop=True, inplace=True) #Reset the index\n",
    "\n",
    "#Save the resulting DataFrame to a CSV file\n",
    "negative_df.to_csv('negative_cleaned.csv', index=False)\n",
    "\n",
    "numNegative = len(negative_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c5294-9211-4529-89df-b394ad533723",
   "metadata": {},
   "source": [
    "# Class representation is approximately equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3cb77ce-c65a-464b-abdb-fb488bdfe1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13697, 13696)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numPositive, numNegative"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
